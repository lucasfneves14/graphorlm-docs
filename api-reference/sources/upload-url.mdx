---
title: 'Scrape Web Page by URL'
description: 'Ingest the textual content of a public web page by providing its URL'
---

Use this endpoint to ingest content by scraping a public web page. It fetches the page, extracts text, and creates a source in your project for downstream processing.

## Endpoint Overview

<CardGroup cols={2}>
  <Card title="HTTP Method" icon="arrow-up">
    **POST**
  </Card>
  <Card title="Endpoint URL" icon="link">
    **https://sources.graphorlm.com/upload-url-source**
  </Card>
</CardGroup>

## Authentication

This endpoint requires authentication using an API token. Include your API token as a Bearer token in the Authorization header.

<Note>
  Learn how to create and manage API tokens in the [API Tokens guide](/guides/api-tokens).
</Note>

## Request Format

### Headers

| Header | Value | Required |
|--------|-------|----------|
| `Authorization` | `Bearer YOUR_API_TOKEN` | ✅ Yes |
| `Content-Type` | `application/json` | ✅ Yes |

### Request Body

Send a JSON body with the following fields:

| Field | Type | Description | Required |
|-------|------|-------------|----------|
| `url` | string | The URL of the web page to scrape | ✅ Yes |
| `crawlUrls` | boolean | Whether to crawl and ingest links from the given URL | No (default: `false`) |

### URL Requirements

<AccordionGroup>
  <Accordion icon="link" title="Supported URL types">
    - Public web pages
    - Pages that render primary content server-side and are reachable without interaction
  </Accordion>
  
  <Accordion icon="shield" title="Access requirements">
    - The URL must be publicly reachable over HTTPS
    - Authentication-protected pages are not supported by this endpoint
  </Accordion>
</AccordionGroup>

<Note>
  This endpoint scrapes web pages. To ingest files (PDF, DOCX, etc.), use the [Upload File](/api-reference/sources/upload) endpoint.
</Note>

## Response Format

### Success Response (200 OK)

```json
{
  "status": "Processing",
  "message": "Source processed successfully",
  "file_name": "https://example.com/",
  "file_size": 0,
  "file_type": "",
  "file_source": "url",
  "project_id": "550e8400-e29b-41d4-a716-446655440000",
  "project_name": "My Project",
  "partition_method": "basic"
}
```

### Response Fields

| Field | Type | Description |
|-------|------|-------------|
| `status` | string | Processing status (`New`, `Processing`, `Completed`, `Failed`, etc.) |
| `message` | string | Human-readable status message |
| `file_name` | string | Name or URL for the ingested source |
| `file_size` | integer | Size in bytes (0 for URL-based initial record) |
| `file_type` | string | Detected type when applicable |
| `file_source` | string | Source type (`url`) |
| `project_id` | string | UUID of the project |
| `project_name` | string | Name of the project |
| `partition_method` | string | Document processing method used |

## Code Examples

### JavaScript/Node.js

```javascript
import fetch from 'node-fetch';

const uploadUrlSource = async (apiToken, url, crawlUrls = false) => {
  const response = await fetch('https://sources.graphorlm.com/upload-url-source', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiToken}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ url, crawlUrls })
  });

  if (!response.ok) {
    throw new Error(`Upload from URL failed: ${response.status} ${response.statusText}`);
  }

  const result = await response.json();
  console.log('URL upload accepted:', result);
  return result;
};

// Usage (scrapes the page content)
uploadUrlSource('grlm_your_api_token_here', 'https://example.com/');
```

### Python

```python
import requests

def upload_url_source(api_token, url, crawl_urls=False):
    endpoint = "https://sources.graphorlm.com/upload-url-source"
    headers = {"Authorization": f"Bearer {api_token}", "Content-Type": "application/json"}
    payload = {"url": url, "crawlUrls": crawl_urls}

    response = requests.post(endpoint, headers=headers, json=payload, timeout=300)
    response.raise_for_status()
    return response.json()

# Usage (scrapes the page content)
result = upload_url_source("grlm_your_api_token_here", "https://example.com")
print("URL scraping accepted:", result["file_name"])  # typically echoes the URL
```

### cURL

```bash
curl -X POST https://sources.graphorlm.com/upload-url-source \
  -H "Authorization: Bearer grlm_your_api_token_here" \
  -H "Content-Type: application/json" \
  -d '{"url":"https://example.com/","crawlUrls":false}'
```

## Error Responses

### Common Error Codes

| Status Code | Error Type | Description |
|-------------|------------|-------------|
| `400` | Bad Request | Invalid or missing URL, malformed JSON |
| `401` | Unauthorized | Invalid or missing API token |
| `403` | Forbidden | Access denied to the specified project |
| `404` | Not Found | Project or source not found |
| `500` | Internal Server Error | Error during URL processing |

### Error Response Format

```json
{
  "detail": "Invalid input: URL is required"
}
```

### Error Examples

<AccordionGroup>
  <Accordion icon="link-slash" title="Invalid URL (400)">
    ```json
    { "detail": "Invalid input: URL is required" }
    ```
  </Accordion>
  
  <Accordion icon="key" title="Invalid API Token (401)">
    ```json
    { "detail": "Invalid authentication credentials" }
    ```
  </Accordion>
</AccordionGroup>

## Document Processing

After a successful request, Graphor begins fetching and scraping the web page in the background.

### Processing Stages

1. **URL Accepted** - The request is validated and scheduled
2. **Content Retrieval** - The page is fetched over HTTPS
3. **Text Extraction** - Visible text is extracted and normalized
4. **Structure Recognition** - Document elements are identified and classified
5. **Ready for Use** - Document is available for chunking and retrieval

### Processing Methods

The system selects the optimal processing method based on the detected content. You can reprocess with a different method after ingestion.

<Note>
  You can reprocess sources using the [Process Source](/api-reference/sources/process) endpoint after ingestion.
</Note>

## Best Practices

- **Provide reachable URLs**: Ensure the page is publicly accessible over HTTPS
- **Disable crawling when unneeded**: Set `crawlUrls` to `false` to ingest only the provided URL
- **Respect site policies**: Only scrape pages you are permitted to and consider website rate limits
- **Retry logic**: Implement retries for transient network issues

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Process Source"
    icon="gears"
    href="/api-reference/sources/process"
  >
    Reprocess with different parsing methods for optimal results
  </Card>
  <Card
    title="List Sources"
    icon="list"
    href="/api-reference/sources/list"
  >
    Retrieve information about all uploaded documents in your project
  </Card>
  <Card
    title="Upload File"
    icon="arrow-up"
    href="/api-reference/sources/upload"
  >
    Ingest local files (PDF, DOCX, etc.)
  </Card>
  <Card
    title="Chunking"
    icon="file-dashed-line"
    href="/guides/chunking"
  >
    Learn how to optimize document segmentation for your RAG pipeline
  </Card>
  <Card
    title="Delete Source"
    icon="trash"
    href="/api-reference/sources/delete"
  >
    Remove documents that are no longer needed from your project
  </Card>
</CardGroup>
