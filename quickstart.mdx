---
title: 'Quickstart'
description: 'Build your first RAG pipeline in minutes'
---

This quickstart gets you up and running with Graphor as quickly as possible. It introduces you to the platform's interface and core RAG pipeline components without diving into technical details or complex configurations.

In this tutorial, you will:
- Load your data into Graphor
- Configure a basic RAG pipeline with intelligent chunking
- Deploy your pipeline

## Step one: Set up Graphor Project

Graphor Cloud provides a fully-managed environment where you can build, test, and deploy RAG pipelines without worrying about infrastructure or complex setup. The free trial gives you full access to all platform features.

1. Sign up for a [Graphor Cloud account](https://app.graphorlm.com/register)
2. Login to your account [Login](https://app.graphorlm.com/login)
2. Once logged in, you'll see the Graphor dashboard
3. Select **New project** and name it as you want

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/project-list-demo.png"
  alt="Project list demo"
  loading="lazy"
/>

Once your project is created, you'll be taken to the sources dashboard where you can begin building your RAG pipeline.

## Step two: Load your data

Graphor allows you to easily upload your own documents to use in your RAG pipeline.

1. In your project, navigate to **Sources** in the left sidebar
2. Click **Add Sources** to open the data upload interface
3. Choose your preferred import method:
   - **Local files**: Upload files from your computer (PDF, MD, PNG, JPG, DOCX, TXT, CSV and others are supported)
   - **URL**: Import content directly from a web address
4. Select or drag and drop your files into the upload area
5. Click **Finish** to begin the ingestion process

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/sources-demo.png"
  alt="Sources demo"
  loading="lazy"
/>

During processing, Graphor will automatically:
- Extract text using advanced OCR for images and scanned documents
- Identify document structure and metadata
- Prepare your content for chunking in the pipeline

You can monitor the ingestion progress on the Sources dashboard.

After upload, you can click on any file to:
- Customize the parsing method and configure element classification to improve document structure recognition
- Preview the extracted content before pipeline processing

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/parsing-demo.png"
  alt="Parsing demo"
  loading="lazy"
/>

## Step three: Create a RAG pipeline

Now let's create a RAG flow to process your documents and build an intelligent question-answering system.

1. In the left sidebar, click **Flows** to navigate to the Flows dashboard
2. Click **New flow** to start creating your pipeline
3. Enter a descriptive name for your flow (e.g., "DocumentQA")
4. Click **Create** to open the visual flow builder

You'll see the visual flow builder interface with components that can be connected to form your RAG pipeline:

- **Dataset**: Select the documents you uploaded in the previous step
- **Chunking**: Configure how your documents are segmented for optimal retrieval
- **Retrieval**: Define how relevant information is searched and ranked
- **Testset**: Create evaluation scenarios with ground-truth answers
- **Questions**: Define sample queries to test your pipeline
- **LLM**: Connect to language models for generating responses
- **Evaluation**: Measure the quality and accuracy of your pipeline
- **Response**: View the final output of your RAG system

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/blank-flow-demo.png"
  alt="Blank flow demo"
  loading="lazy"
/>

The flow builder uses a drag-and-drop interface where you can connect these components to create your custom RAG pipeline.

## Step four: Configure pipeline components

Now let's build your RAG pipeline by adding and connecting the essential components:

1. Add the **Dataset** component
   - Drag the Dataset component from the sidebar onto the canvas
   - Double-click on the component to open its configuration panel
   - Select the sources you imported earlier from the files menu
   - Close the configuration panel
   - This component provides your documents to the pipeline

2. Add the **Chunking** component
   - Drag the Chunking component onto the canvas
   - Connect the output of the Dataset component to the input of the Chunking component
   - Double-click to configure with these recommended settings:
     - **Embedding / Indexer**: text-embedding-3-small (provides semantic understanding)
     - **Elements to remove**: None (or select elements to exclude)
     - **Splitter**: Smart chunking (maintains semantic coherence)
     - **Chunk Size**: 5000 (adjust based on your document complexity)
   - Close the configuration panel
   - This component divides your documents into smaller pieces for efficient retrieval

3. Add the **Retrieval** component
   - Drag the Retrieval component onto the canvas
   - Connect the output of the Chunking component to the input of the Retrieval
   - Double-click to configure:
     - **Search Type**: Similarity (recommended for semantic relevance)
     - **Top K**: 5 (number of chunks to retrieve)
     - **Score Threshold**: 0.5 (minimum relevance score)
   - Close the configuration panel
   - This component searches your chunked documents for relevant information

4. Add the **Response** component
   - Drag the Response component onto the canvas
   - Connect the output of the Retrieval component to the input of the Response
   - This component formats and displays the final output of your pipeline

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/simple-flow-demo.png"
  alt="Simple flow demo"
  loading="lazy"
/>

After configuring each component, you can test individual nodes by:
- Double-clicking the component to open its configuration panel
- Clicking **Update results** to see the component's output based on current settings
- Examining the processed data to verify it meets your expectations

**Important Note**: To fully simulate a flow with retrieval nodes, you must add either:
- A **Question** node: For testing individual queries
- A **Testset** node: For batch testing multiple questions

These input nodes provide the questions that drive the retrieval process in your RAG pipeline.

## Step five: Deploy your RAG pipeline

After building and testing your flow, it's time to deploy it for production use. Deployment makes your RAG pipeline accessible via API and MCP ([Model Context Protocol](https://modelcontextprotocol.io/)), allowing integration with applications, websites, or other systems.

To deploy your pipeline:

1. Click on the **Deploy new revision** button in the top-right corner of the flow builder
2. In the deployment modal, add a detailed **Tool description for MCP Server**:
   - This description helps LLMs understand when and how to use your flow
   - Write a clear, concise explanation of what questions your flow can answer
   - Example: "This tool answers questions about technical documentation and product specifications"
3. Under **Traffic Configuration**:
   - Select the checkbox to display this revision immediately
   - This sets 100% of traffic to use this revision, replacing any existing traffic splits
4. Click **Create** to deploy the revision

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/new-revision-demo.png"
  alt="New revision demo"
  loading="lazy"
/>

Congratulations! Your pipeline is now online.

## Final step: Connect to your flow

Click the **Connect to flow** button to view integration options:
   - **REST API**: Direct HTTP endpoint for custom applications
   - **MCP Server**: Model Context Protocol integration for LLM tools

Your RAG pipeline is now ready for production use with real users and data!

You can test your online pipeline by clicking on the chat button at the bottom right corner of the Graphor interface. This built-in chat interface allows you to:
- Send questions directly to your deployed flow
- View retrieved context from your documents
- Experience the pipeline from an end-user perspective
- Verify that your deployment is working correctly

## Next steps

Congratulations! You've built and tested your first RAG pipeline with Graphor. Here's what you can explore next:

<CardGroup cols={2}>
  <Card
    title="Integrate Workflow"
    icon="plug"
    href="guides/integrate-workflow"
  >
    Connect your RAG systems to applications via REST API and MCP Server integration
  </Card>
  <Card
    title="Data Ingestion"
    icon="file-lines"
    href="guides/data-ingestion"
  >
    Transform unstructured documents into processable data using advanced OCR and classification
  </Card>
  <Card
    title="Chunking"
    icon="file-dashed-line"
    href="guides/chunking"
  >
    Optimize document segmentation with intelligent chunking for maximum retrieval relevance
  </Card>
  <Card
    title="Evaluation"
    icon="square-poll-vertical"
    href="guides/evaluation"
  >
    Measure and improve your RAG pipeline performance with comprehensive metrics
  </Card>
</CardGroup>
